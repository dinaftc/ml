{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac1333f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment:confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason:confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>681448150</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 5:24</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:35</td>\n",
       "      <td>5.703060e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>681448153</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 1:53</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>681448156</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 10:01</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681448158</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 3:05</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>681448159</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/25/15 5:50</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/15 11:14</td>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  681448150    False   finalized                   3      2/25/15 5:24   \n",
       "1  681448153    False   finalized                   3      2/25/15 1:53   \n",
       "2  681448156    False   finalized                   3     2/25/15 10:01   \n",
       "3  681448158    False   finalized                   3      2/25/15 3:05   \n",
       "4  681448159    False   finalized                   3      2/25/15 5:50   \n",
       "\n",
       "  airline_sentiment  airline_sentiment:confidence negativereason  \\\n",
       "0           neutral                        1.0000            NaN   \n",
       "1          positive                        0.3486            NaN   \n",
       "2           neutral                        0.6837            NaN   \n",
       "3          negative                        1.0000     Bad Flight   \n",
       "4          negative                        1.0000     Can't Tell   \n",
       "\n",
       "   negativereason:confidence         airline airline_sentiment_gold  \\\n",
       "0                        NaN  Virgin America                    NaN   \n",
       "1                     0.0000  Virgin America                    NaN   \n",
       "2                        NaN  Virgin America                    NaN   \n",
       "3                     0.7033  Virgin America                    NaN   \n",
       "4                     1.0000  Virgin America                    NaN   \n",
       "\n",
       "         name negativereason_gold  retweet_count  \\\n",
       "0     cairdin                 NaN              0   \n",
       "1    jnardino                 NaN              0   \n",
       "2  yvonnalynn                 NaN              0   \n",
       "3    jnardino                 NaN              0   \n",
       "4    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "   tweet_created      tweet_id tweet_location               user_timezone  \n",
       "0  2/24/15 11:35  5.703060e+17            NaN  Eastern Time (US & Canada)  \n",
       "1  2/24/15 11:15  5.703010e+17            NaN  Pacific Time (US & Canada)  \n",
       "2  2/24/15 11:15  5.703010e+17      Lets Play  Central Time (US & Canada)  \n",
       "3  2/24/15 11:15  5.703010e+17            NaN  Pacific Time (US & Canada)  \n",
       "4  2/24/15 11:14  5.703010e+17            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "import demoji\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re, string\n",
    "import emoji\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = pd.read_csv('Airline-Sentiment-2-w-AA.csv', encoding='ISO-8859-1')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab2b9229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 20 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   _unit_id                      14640 non-null  int64  \n",
      " 1   _golden                       14640 non-null  bool   \n",
      " 2   _unit_state                   14640 non-null  object \n",
      " 3   _trusted_judgments            14640 non-null  int64  \n",
      " 4   _last_judgment_at             14584 non-null  object \n",
      " 5   airline_sentiment             14640 non-null  object \n",
      " 6   airline_sentiment:confidence  14640 non-null  float64\n",
      " 7   negativereason                9178 non-null   object \n",
      " 8   negativereason:confidence     10522 non-null  float64\n",
      " 9   airline                       14640 non-null  object \n",
      " 10  airline_sentiment_gold        40 non-null     object \n",
      " 11  name                          14640 non-null  object \n",
      " 12  negativereason_gold           32 non-null     object \n",
      " 13  retweet_count                 14640 non-null  int64  \n",
      " 14  text                          14640 non-null  object \n",
      " 15  tweet_coord                   1019 non-null   object \n",
      " 16  tweet_created                 14640 non-null  object \n",
      " 17  tweet_id                      14640 non-null  float64\n",
      " 18  tweet_location                9907 non-null   object \n",
      " 19  user_timezone                 9820 non-null   object \n",
      "dtypes: bool(1), float64(3), int64(3), object(13)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.airline_sentiment.unique()\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6162d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                 0\n",
       "airline_sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data[['text','airline_sentiment']]\n",
    "df.head()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e28bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean  text\n",
    "def clean_text_from_emojis(text):\n",
    "    return demoji.replace(text, '')\n",
    "\n",
    "#Remove punctuations, links, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text):\n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "# Clean hastags '#' Symbol\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
    "    return new_tweet2\n",
    "\n",
    "# Remove multiple spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "\n",
    "\n",
    "texts_new = []\n",
    "for t in df.text:\n",
    "    texts_new.append(remove_mult_spaces(clean_hashtags(strip_all_entities(clean_text_from_emojis(t)))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9aaf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ST\\AppData\\Local\\Temp\\ipykernel_18592\\2826292267.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_text'] = texts_new\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>what said</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus youve added commercials to the experience...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>i didnt today must mean i need to take another...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>its really aggressive to blast obnoxious enter...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                @VirginAmerica What @dhepburn said.   \n",
       "1  @VirginAmerica plus you've added commercials t...   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3  @VirginAmerica it's really aggressive to blast...   \n",
       "4  @VirginAmerica and it's a really big bad thing...   \n",
       "\n",
       "                                        cleaned_text airline_sentiment  \n",
       "0                                          what said           neutral  \n",
       "1  plus youve added commercials to the experience...          positive  \n",
       "2  i didnt today must mean i need to take another...           neutral  \n",
       "3  its really aggressive to blast obnoxious enter...          negative  \n",
       "4            and its a really big bad thing about it          negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text'] = texts_new\n",
    "\n",
    "# rearrange the column order\n",
    "df = df.reindex(columns=['text','cleaned_text', 'airline_sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e071bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "0                    @VirginAmerica What @dhepburn said.   \n",
      "1      @VirginAmerica plus you've added commercials t...   \n",
      "2      @VirginAmerica I didn't today... Must mean I n...   \n",
      "3      @VirginAmerica it's really aggressive to blast...   \n",
      "4      @VirginAmerica and it's a really big bad thing...   \n",
      "...                                                  ...   \n",
      "14635  @AmericanAir thank you we got on a different f...   \n",
      "14636  @AmericanAir leaving over 20 minutes Late Flig...   \n",
      "14637  @AmericanAir Please bring American Airlines to...   \n",
      "14638  @AmericanAir you have my money, you change my ...   \n",
      "14639  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
      "\n",
      "                                            cleaned_text airline_sentiment  \n",
      "0                                              what said           neutral  \n",
      "1      plus youve added commercials to the experience...              None  \n",
      "2      i didnt today must mean i need to take another...           neutral  \n",
      "3      its really aggressive to blast obnoxious enter...          negative  \n",
      "4                and its a really big bad thing about it          negative  \n",
      "...                                                  ...               ...  \n",
      "14635  thank you we got on a different flight to chicago          positive  \n",
      "14636  leaving over 20 minutes late flight no warning...              None  \n",
      "14637     please bring american airlines to blackberry10           neutral  \n",
      "14638  you have my money you change my flight and don...              None  \n",
      "14639  we have 8 ppl so we need 2 know how many seats...           neutral  \n",
      "\n",
      "[14640 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a portion of the data to turn into unlabeled\n",
    "percentage_to_turn_unlabeled = 0.4  # Adjust as needed\n",
    "num_samples_to_turn_unlabeled = int(len(df) * percentage_to_turn_unlabeled)\n",
    "samples_to_turn_unlabeled = df.sample(num_samples_to_turn_unlabeled)\n",
    "\n",
    "# Set the 'label' column for the selected samples to NaN\n",
    "df.loc[samples_to_turn_unlabeled.index, 'airline_sentiment'] = None  # or df.loc[samples_to_turn_unlabeled.index, 'label'] = np.nan\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)\n",
    "labeled_data = df[df['airline_sentiment'].notnull()]\n",
    "unlabeled_data = df[df['airline_sentiment'].isnull()]\n",
    "# Instantiate the vectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3746497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 79.34%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1047   46   17]\n",
      " [ 153  207   17]\n",
      " [  81   49  140]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.94      0.88      1110\n",
      "     neutral       0.69      0.55      0.61       377\n",
      "    positive       0.80      0.52      0.63       270\n",
      "\n",
      "    accuracy                           0.79      1757\n",
      "   macro avg       0.77      0.67      0.71      1757\n",
      "weighted avg       0.79      0.79      0.78      1757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ST\\AppData\\Local\\Temp\\ipykernel_18592\\413566608.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unlabeled_data['predicted_sentiment'] = predicted_sentiments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set after retraining: 91.41%\n",
      "\n",
      "Confusion Matrix after retraining:\n",
      "[[1094   10    6]\n",
      " [  73  297    7]\n",
      " [  28   27  215]]\n",
      "\n",
      "Classification Report after retraining:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.99      0.95      1110\n",
      "     neutral       0.89      0.79      0.84       377\n",
      "    positive       0.94      0.80      0.86       270\n",
      "\n",
      "    accuracy                           0.91      1757\n",
      "   macro avg       0.92      0.86      0.88      1757\n",
      "weighted avg       0.91      0.91      0.91      1757\n",
      "\n",
      "                                                    text predicted_sentiment\n",
      "1      @VirginAmerica plus you've added commercials t...            negative\n",
      "5      @VirginAmerica seriously would pay $30 a fligh...            negative\n",
      "10     @VirginAmerica did you know that suicide is th...             neutral\n",
      "15         @VirginAmerica SFO-PDX schedule is still MIA.            negative\n",
      "16     @VirginAmerica So excited for my first cross c...            positive\n",
      "...                                                  ...                 ...\n",
      "14626  @AmericanAir Flight 953 NYC-Buenos Aires has b...            negative\n",
      "14630                        @AmericanAir Thanks! He is.            positive\n",
      "14632  ÛÏ@AmericanAir: @TilleyMonsta George, that do...            negative\n",
      "14636  @AmericanAir leaving over 20 minutes Late Flig...            negative\n",
      "14638  @AmericanAir you have my money, you change my ...            negative\n",
      "\n",
      "[5856 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clean and prepare the DataFrame\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: remove_mult_spaces(clean_hashtags(strip_all_entities(clean_text_from_emojis(x)))))\n",
    "\n",
    "# Separate labeled and unlabeled data\n",
    "labeled_data = df[df['airline_sentiment'].notnull()]\n",
    "unlabeled_data = df[df['airline_sentiment'].isnull()]\n",
    "\n",
    "# Separate the features (X) and labels (y)\n",
    "X_labeled = vectorizer.fit_transform(labeled_data['cleaned_text'])\n",
    "y_labeled = labeled_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable\n",
    "mask_labeled = y_labeled.notnull()\n",
    "X_labeled = X_labeled[mask_labeled]\n",
    "y_labeled = y_labeled[mask_labeled]\n",
    "\n",
    "# Split the labeled data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the initial classifier on the labeled training data\n",
    "classifier1 = SVC(kernel='linear')\n",
    "classifier1.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "predicted_labels = classifier1.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the test set\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "class_report = classification_report(y_test, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.2%}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Use TF-IDF to convert text to numerical features for the unlabeled data\n",
    "X_unlabeled = vectorizer.transform(unlabeled_data['cleaned_text'])\n",
    "\n",
    "# Predict sentiment on the unlabeled data\n",
    "predicted_sentiments = classifier1.predict(X_unlabeled)\n",
    "\n",
    "# Add predicted sentiments to the unlabeled dataset\n",
    "unlabeled_data['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "# Combine labeled and unlabeled data\n",
    "combined_data = pd.concat([labeled_data, unlabeled_data])\n",
    "\n",
    "# Separate features and labels for the combined data\n",
    "X_combined = vectorizer.transform(combined_data['cleaned_text'])\n",
    "y_combined = combined_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable for combined data\n",
    "mask_combined = y_combined.notnull()\n",
    "X_combined = X_combined[mask_combined]\n",
    "y_combined = y_combined[mask_combined]\n",
    "\n",
    "# Retrain the classifier on the combined data\n",
    "classifier1.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict the labels on the test data again (for evaluation)\n",
    "predicted_labels_combined = classifier1.predict(X_test)\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the retrained model\n",
    "accuracy_combined = accuracy_score(y_test, predicted_labels_combined)\n",
    "conf_matrix_combined = confusion_matrix(y_test, predicted_labels_combined)\n",
    "class_report_combined = classification_report(y_test, predicted_labels_combined)\n",
    "\n",
    "print(f\"Accuracy on the test set after retraining: {accuracy_combined:.2%}\")\n",
    "print(\"\\nConfusion Matrix after retraining:\")\n",
    "print(conf_matrix_combined)\n",
    "print(\"\\nClassification Report after retraining:\")\n",
    "print(class_report_combined)\n",
    "\n",
    "# Display the result for the unlabeled data\n",
    "print(unlabeled_data[['text', 'predicted_sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b508ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment for the new text: negative\n"
     ]
    }
   ],
   "source": [
    "# Example of new text to test\n",
    "new_text = \"I am not happy with the service\"\n",
    "\n",
    "# Clean and preprocess the new text\n",
    "cleaned_new_text = remove_mult_spaces(clean_hashtags(strip_all_entities(clean_text_from_emojis(new_text))))\n",
    "\\\n",
    "# Transform the preprocessed text using the TF-IDF vectorizer\n",
    "X_new = vectorizer.transform([cleaned_new_text])\n",
    "\n",
    "# Use the trained model to predict the sentiment\n",
    "predicted_sentiment = classifier1.predict(X_new)[0]\n",
    "\n",
    "# Display the result\n",
    "print(f\"Predicted sentiment for the new text: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafc7ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 77.97%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1058   38   14]\n",
      " [ 175  188   14]\n",
      " [  97   49  124]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.95      0.87      1110\n",
      "     neutral       0.68      0.50      0.58       377\n",
      "    positive       0.82      0.46      0.59       270\n",
      "\n",
      "    accuracy                           0.78      1757\n",
      "   macro avg       0.76      0.64      0.68      1757\n",
      "weighted avg       0.77      0.78      0.76      1757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ST\\AppData\\Local\\Temp\\ipykernel_18592\\1739715095.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unlabeled_data['predicted_sentiment'] = predicted_sentiments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set after retraining: 88.56%\n",
      "\n",
      "Confusion Matrix after retraining:\n",
      "[[1089   14    7]\n",
      " [  85  287    5]\n",
      " [  55   35  180]]\n",
      "\n",
      "Classification Report after retraining:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93      1110\n",
      "     neutral       0.85      0.76      0.81       377\n",
      "    positive       0.94      0.67      0.78       270\n",
      "\n",
      "    accuracy                           0.89      1757\n",
      "   macro avg       0.89      0.80      0.84      1757\n",
      "weighted avg       0.89      0.89      0.88      1757\n",
      "\n",
      "                                                    text predicted_sentiment\n",
      "1      @VirginAmerica plus you've added commercials t...            negative\n",
      "5      @VirginAmerica seriously would pay $30 a fligh...            negative\n",
      "10     @VirginAmerica did you know that suicide is th...             neutral\n",
      "15         @VirginAmerica SFO-PDX schedule is still MIA.            negative\n",
      "16     @VirginAmerica So excited for my first cross c...            positive\n",
      "...                                                  ...                 ...\n",
      "14626  @AmericanAir Flight 953 NYC-Buenos Aires has b...            negative\n",
      "14630                        @AmericanAir Thanks! He is.            positive\n",
      "14632  ÛÏ@AmericanAir: @TilleyMonsta George, that do...             neutral\n",
      "14636  @AmericanAir leaving over 20 minutes Late Flig...            negative\n",
      "14638  @AmericanAir you have my money, you change my ...            negative\n",
      "\n",
      "[5856 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ST\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare the DataFrame\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: remove_mult_spaces(clean_hashtags(strip_all_entities(clean_text_from_emojis(x)))))\n",
    "\n",
    "# Separate labeled and unlabeled data\n",
    "labeled_data = df[df['airline_sentiment'].notnull()]\n",
    "unlabeled_data = df[df['airline_sentiment'].isnull()]\n",
    "\n",
    "# Separate the features (X) and labels (y)\n",
    "X_labeled = vectorizer.fit_transform(labeled_data['cleaned_text'])\n",
    "y_labeled = labeled_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable\n",
    "mask_labeled = y_labeled.notnull()\n",
    "X_labeled = X_labeled[mask_labeled]\n",
    "y_labeled = y_labeled[mask_labeled]\n",
    "\n",
    "# Split the labeled data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the initial classifier on the labeled training data\n",
    "classifier2 = LogisticRegression()\n",
    "classifier2.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "predicted_labels = classifier2.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the test set\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "class_report = classification_report(y_test, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.2%}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Use TF-IDF to convert text to numerical features for the unlabeled data\n",
    "X_unlabeled = vectorizer.transform(unlabeled_data['cleaned_text'])\n",
    "\n",
    "# Predict sentiment on the unlabeled data\n",
    "predicted_sentiments = classifier2.predict(X_unlabeled)\n",
    "\n",
    "# Add predicted sentiments to the unlabeled dataset\n",
    "unlabeled_data['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "# Combine labeled and unlabeled data\n",
    "combined_data = pd.concat([labeled_data, unlabeled_data])\n",
    "\n",
    "# Separate features and labels for the combined data\n",
    "X_combined = vectorizer.transform(combined_data['cleaned_text'])\n",
    "y_combined = combined_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable for combined data\n",
    "mask_combined = y_combined.notnull()\n",
    "X_combined = X_combined[mask_combined]\n",
    "y_combined = y_combined[mask_combined]\n",
    "\n",
    "# Retrain the classifier on the combined data\n",
    "classifier2.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict the labels on the test data again (for evaluation)\n",
    "predicted_labels_combined = classifier2.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the retrained model\n",
    "accuracy_combined = accuracy_score(y_test, predicted_labels_combined)\n",
    "conf_matrix_combined = confusion_matrix(y_test, predicted_labels_combined)\n",
    "class_report_combined = classification_report(y_test, predicted_labels_combined)\n",
    "\n",
    "print(f\"Accuracy on the test set after retraining: {accuracy_combined:.2%}\")\n",
    "print(\"\\nConfusion Matrix after retraining:\")\n",
    "print(conf_matrix_combined)\n",
    "print(\"\\nClassification Report after retraining:\")\n",
    "print(class_report_combined)\n",
    "\n",
    "# Display the result for the unlabeled data\n",
    "print(unlabeled_data[['text', 'predicted_sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab83566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 74.39%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1075   25   10]\n",
      " [ 230  137   10]\n",
      " [ 141   34   95]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.97      0.84      1110\n",
      "     neutral       0.70      0.36      0.48       377\n",
      "    positive       0.83      0.35      0.49       270\n",
      "\n",
      "    accuracy                           0.74      1757\n",
      "   macro avg       0.76      0.56      0.60      1757\n",
      "weighted avg       0.75      0.74      0.71      1757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ST\\AppData\\Local\\Temp\\ipykernel_18592\\3319694217.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unlabeled_data['predicted_sentiment'] = predicted_sentiments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set after retraining: 99.77%\n",
      "\n",
      "Confusion Matrix after retraining:\n",
      "[[1110    0    0]\n",
      " [   0  376    1]\n",
      " [   0    3  267]]\n",
      "\n",
      "Classification Report after retraining:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00      1110\n",
      "     neutral       0.99      1.00      0.99       377\n",
      "    positive       1.00      0.99      0.99       270\n",
      "\n",
      "    accuracy                           1.00      1757\n",
      "   macro avg       1.00      1.00      1.00      1757\n",
      "weighted avg       1.00      1.00      1.00      1757\n",
      "\n",
      "                                                    text predicted_sentiment\n",
      "1      @VirginAmerica plus you've added commercials t...            negative\n",
      "5      @VirginAmerica seriously would pay $30 a fligh...            negative\n",
      "10     @VirginAmerica did you know that suicide is th...             neutral\n",
      "15         @VirginAmerica SFO-PDX schedule is still MIA.            negative\n",
      "16     @VirginAmerica So excited for my first cross c...            negative\n",
      "...                                                  ...                 ...\n",
      "14626  @AmericanAir Flight 953 NYC-Buenos Aires has b...            negative\n",
      "14630                        @AmericanAir Thanks! He is.            positive\n",
      "14632  ÛÏ@AmericanAir: @TilleyMonsta George, that do...            negative\n",
      "14636  @AmericanAir leaving over 20 minutes Late Flig...            negative\n",
      "14638  @AmericanAir you have my money, you change my ...            negative\n",
      "\n",
      "[5856 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare the DataFrame\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: remove_mult_spaces(clean_hashtags(strip_all_entities(clean_text_from_emojis(x)))))\n",
    "\n",
    "# Separate labeled and unlabeled data\n",
    "labeled_data = df[df['airline_sentiment'].notnull()]\n",
    "unlabeled_data = df[df['airline_sentiment'].isnull()]\n",
    "\n",
    "# Separate the features (X) and labels (y)\n",
    "X_labeled = vectorizer.fit_transform(labeled_data['cleaned_text'])\n",
    "y_labeled = labeled_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable\n",
    "mask_labeled = y_labeled.notnull()\n",
    "X_labeled = X_labeled[mask_labeled]\n",
    "y_labeled = y_labeled[mask_labeled]\n",
    "\n",
    "# Split the labeled data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the initial classifier on the labeled training data\n",
    "classifier3 = RandomForestClassifier()\n",
    "classifier3.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "predicted_labels = classifier3.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the test set\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "class_report = classification_report(y_test, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.2%}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Use TF-IDF to convert text to numerical features for the unlabeled data\n",
    "X_unlabeled = vectorizer.transform(unlabeled_data['cleaned_text'])\n",
    "\n",
    "# Predict sentiment on the unlabeled data\n",
    "predicted_sentiments = classifier3.predict(X_unlabeled)\n",
    "\n",
    "# Add predicted sentiments to the unlabeled dataset\n",
    "unlabeled_data['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "# Combine labeled and unlabeled data\n",
    "combined_data = pd.concat([labeled_data, unlabeled_data])\n",
    "\n",
    "# Separate features and labels for the combined data\n",
    "X_combined = vectorizer.transform(combined_data['cleaned_text'])\n",
    "y_combined = combined_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable for combined data\n",
    "mask_combined = y_combined.notnull()\n",
    "X_combined = X_combined[mask_combined]\n",
    "y_combined = y_combined[mask_combined]\n",
    "\n",
    "# Retrain the classifier on the combined data\n",
    "classifier3.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict the labels on the test data again (for evaluation)\n",
    "predicted_labels_combined = classifier3.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the retrained model\n",
    "accuracy_combined = accuracy_score(y_test, predicted_labels_combined)\n",
    "conf_matrix_combined = confusion_matrix(y_test, predicted_labels_combined)\n",
    "class_report_combined = classification_report(y_test, predicted_labels_combined)\n",
    "\n",
    "print(f\"Accuracy on the test set after retraining: {accuracy_combined:.2%}\")\n",
    "print(\"\\nConfusion Matrix after retraining:\")\n",
    "print(conf_matrix_combined)\n",
    "print(\"\\nClassification Report after retraining:\")\n",
    "print(class_report_combined)\n",
    "\n",
    "# Display the result for the unlabeled data\n",
    "print(unlabeled_data[['text', 'predicted_sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "024f6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment for the new text: negative\n"
     ]
    }
   ],
   "source": [
    "# Example of new text to test\n",
    "new_text = \"I am pleased with the service\"\n",
    "\n",
    "# Clean and preprocess the new text\n",
    "cleaned_new_text = remove_mult_spaces(clean_hashtags(strip_all_entities(clean_text_from_emojis(new_text))))\n",
    "\\\n",
    "# Transform the preprocessed text using the TF-IDF vectorizer\n",
    "X_new = vectorizer.transform([cleaned_new_text])\n",
    "\n",
    "# Use the trained model to predict the sentiment\n",
    "predicted_sentiment = classifier3.predict(X_new)[0]\n",
    "\n",
    "# Display the result\n",
    "print(f\"Predicted sentiment for the new text: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86500ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 72.97%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1070   18   22]\n",
      " [ 276   75   26]\n",
      " [ 123   10  137]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.96      0.83      1110\n",
      "     neutral       0.73      0.20      0.31       377\n",
      "    positive       0.74      0.51      0.60       270\n",
      "\n",
      "    accuracy                           0.73      1757\n",
      "   macro avg       0.73      0.56      0.58      1757\n",
      "weighted avg       0.73      0.73      0.68      1757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ST\\AppData\\Local\\Temp\\ipykernel_18592\\2480138799.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unlabeled_data['predicted_sentiment'] = predicted_sentiments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set after retraining: 77.06%\n",
      "\n",
      "Confusion Matrix after retraining:\n",
      "[[1086    8   16]\n",
      " [ 239  118   20]\n",
      " [ 113    7  150]]\n",
      "\n",
      "Classification Report after retraining:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.98      0.85      1110\n",
      "     neutral       0.89      0.31      0.46       377\n",
      "    positive       0.81      0.56      0.66       270\n",
      "\n",
      "    accuracy                           0.77      1757\n",
      "   macro avg       0.82      0.62      0.66      1757\n",
      "weighted avg       0.79      0.77      0.74      1757\n",
      "\n",
      "                                                    text predicted_sentiment\n",
      "1      @VirginAmerica plus you've added commercials t...            negative\n",
      "5      @VirginAmerica seriously would pay $30 a fligh...            negative\n",
      "10     @VirginAmerica did you know that suicide is th...            negative\n",
      "15         @VirginAmerica SFO-PDX schedule is still MIA.            negative\n",
      "16     @VirginAmerica So excited for my first cross c...            positive\n",
      "...                                                  ...                 ...\n",
      "14626  @AmericanAir Flight 953 NYC-Buenos Aires has b...            negative\n",
      "14630                        @AmericanAir Thanks! He is.            positive\n",
      "14632  ÛÏ@AmericanAir: @TilleyMonsta George, that do...            negative\n",
      "14636  @AmericanAir leaving over 20 minutes Late Flig...            negative\n",
      "14638  @AmericanAir you have my money, you change my ...            negative\n",
      "\n",
      "[5856 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare the DataFrame\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: remove_mult_spaces(clean_hashtags(strip_all_entities(clean_text_from_emojis(x)))))\n",
    "\n",
    "# Separate labeled and unlabeled data\n",
    "labeled_data = df[df['airline_sentiment'].notnull()]\n",
    "unlabeled_data = df[df['airline_sentiment'].isnull()]\n",
    "\n",
    "# Separate the features (X) and labels (y)\n",
    "X_labeled = vectorizer.fit_transform(labeled_data['cleaned_text'])\n",
    "y_labeled = labeled_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable\n",
    "mask_labeled = y_labeled.notnull()\n",
    "X_labeled = X_labeled[mask_labeled]\n",
    "y_labeled = y_labeled[mask_labeled]\n",
    "\n",
    "# Split the labeled data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the initial classifier on the labeled training data\n",
    "\n",
    "classifier4 = GradientBoostingClassifier()\n",
    "classifier4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "predicted_labels = classifier4.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the test set\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "class_report = classification_report(y_test, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.2%}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "# Use TF-IDF to convert text to numerical features for the unlabeled data\n",
    "X_unlabeled = vectorizer.transform(unlabeled_data['cleaned_text'])\n",
    "\n",
    "# Predict sentiment on the unlabeled data\n",
    "predicted_sentiments = classifier4.predict(X_unlabeled)\n",
    "\n",
    "# Add predicted sentiments to the unlabeled dataset\n",
    "unlabeled_data['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "# Combine labeled and unlabeled data\n",
    "combined_data = pd.concat([labeled_data, unlabeled_data])\n",
    "\n",
    "# Separate features and labels for the combined data\n",
    "X_combined = vectorizer.transform(combined_data['cleaned_text'])\n",
    "y_combined = combined_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable for combined data\n",
    "mask_combined = y_combined.notnull()\n",
    "X_combined = X_combined[mask_combined]\n",
    "y_combined = y_combined[mask_combined]\n",
    "\n",
    "# Retrain the classifier on the combined data\n",
    "classifier4.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict the labels on the test data again (for evaluation)\n",
    "predicted_labels_combined = classifier4.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the retrained model\n",
    "accuracy_combined = accuracy_score(y_test, predicted_labels_combined)\n",
    "conf_matrix_combined = confusion_matrix(y_test, predicted_labels_combined)\n",
    "class_report_combined = classification_report(y_test, predicted_labels_combined)\n",
    "\n",
    "print(f\"Accuracy on the test set after retraining: {accuracy_combined:.2%}\")\n",
    "print(\"\\nConfusion Matrix after retraining:\")\n",
    "print(conf_matrix_combined)\n",
    "print(\"\\nClassification Report after retraining:\")\n",
    "print(class_report_combined)\n",
    "\n",
    "# Display the result for the unlabeled data\n",
    "print(unlabeled_data[['text', 'predicted_sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e64501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 67.05%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1107    2    1]\n",
      " [ 336   39    2]\n",
      " [ 232    6   32]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      1.00      0.79      1110\n",
      "     neutral       0.83      0.10      0.18       377\n",
      "    positive       0.91      0.12      0.21       270\n",
      "\n",
      "    accuracy                           0.67      1757\n",
      "   macro avg       0.80      0.41      0.40      1757\n",
      "weighted avg       0.74      0.67      0.57      1757\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ST\\AppData\\Local\\Temp\\ipykernel_18592\\4282307648.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unlabeled_data['predicted_sentiment'] = predicted_sentiments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set after retraining: 71.09%\n",
      "\n",
      "Confusion Matrix after retraining:\n",
      "[[1108    1    1]\n",
      " [ 285   91    1]\n",
      " [ 214    6   50]]\n",
      "\n",
      "Classification Report after retraining:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      1.00      0.82      1110\n",
      "     neutral       0.93      0.24      0.38       377\n",
      "    positive       0.96      0.19      0.31       270\n",
      "\n",
      "    accuracy                           0.71      1757\n",
      "   macro avg       0.86      0.47      0.50      1757\n",
      "weighted avg       0.78      0.71      0.65      1757\n",
      "\n",
      "                                                    text predicted_sentiment\n",
      "1      @VirginAmerica plus you've added commercials t...            negative\n",
      "5      @VirginAmerica seriously would pay $30 a fligh...            negative\n",
      "10     @VirginAmerica did you know that suicide is th...             neutral\n",
      "15         @VirginAmerica SFO-PDX schedule is still MIA.            negative\n",
      "16     @VirginAmerica So excited for my first cross c...            negative\n",
      "...                                                  ...                 ...\n",
      "14626  @AmericanAir Flight 953 NYC-Buenos Aires has b...            negative\n",
      "14630                        @AmericanAir Thanks! He is.            negative\n",
      "14632  ÛÏ@AmericanAir: @TilleyMonsta George, that do...            negative\n",
      "14636  @AmericanAir leaving over 20 minutes Late Flig...            negative\n",
      "14638  @AmericanAir you have my money, you change my ...            negative\n",
      "\n",
      "[5856 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare the DataFrame\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: remove_mult_spaces(clean_hashtags(strip_all_entities(clean_text_from_emojis(x)))))\n",
    "\n",
    "# Separate labeled and unlabeled data\n",
    "labeled_data = df[df['airline_sentiment'].notnull()]\n",
    "unlabeled_data = df[df['airline_sentiment'].isnull()]\n",
    "\n",
    "# Separate the features (X) and labels (y)\n",
    "X_labeled = vectorizer.fit_transform(labeled_data['cleaned_text'])\n",
    "y_labeled = labeled_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable\n",
    "mask_labeled = y_labeled.notnull()\n",
    "X_labeled = X_labeled[mask_labeled]\n",
    "y_labeled = y_labeled[mask_labeled]\n",
    "\n",
    "# Split the labeled data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "classifier5 = MultinomialNB()\n",
    "classifier5.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "predicted_labels = classifier5.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the test set\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "class_report = classification_report(y_test, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy:.2%}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Use TF-IDF to convert text to numerical features for the unlabeled data\n",
    "X_unlabeled = vectorizer.transform(unlabeled_data['cleaned_text'])\n",
    "\n",
    "# Predict sentiment on the unlabeled data\n",
    "predicted_sentiments = classifier5.predict(X_unlabeled)\n",
    "\n",
    "# Add predicted sentiments to the unlabeled dataset\n",
    "unlabeled_data['predicted_sentiment'] = predicted_sentiments\n",
    "\n",
    "# Combine labeled and unlabeled data\n",
    "combined_data = pd.concat([labeled_data, unlabeled_data])\n",
    "\n",
    "# Separate features and labels for the combined data\n",
    "X_combined = vectorizer.transform(combined_data['cleaned_text'])\n",
    "y_combined = combined_data['airline_sentiment']\n",
    "\n",
    "# Filter out rows with None values in the target variable for combined data\n",
    "mask_combined = y_combined.notnull()\n",
    "X_combined = X_combined[mask_combined]\n",
    "y_combined = y_combined[mask_combined]\n",
    "\n",
    "# Retrain the classifier on the combined data\n",
    "classifier5.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict the labels on the test data again (for evaluation)\n",
    "predicted_labels_combined = classifier5.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy, confusion matrix, and classification report on the retrained model\n",
    "accuracy_combined = accuracy_score(y_test, predicted_labels_combined)\n",
    "conf_matrix_combined = confusion_matrix(y_test, predicted_labels_combined)\n",
    "class_report_combined = classification_report(y_test, predicted_labels_combined)\n",
    "\n",
    "print(f\"Accuracy on the test set after retraining: {accuracy_combined:.2%}\")\n",
    "print(\"\\nConfusion Matrix after retraining:\")\n",
    "print(conf_matrix_combined)\n",
    "print(\"\\nClassification Report after retraining:\")\n",
    "print(class_report_combined)\n",
    "\n",
    "# Display the result for the unlabeled data\n",
    "print(unlabeled_data[['text', 'predicted_sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf3a3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['randomforest.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "joblib.dump(classifier3, 'randomforest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824d381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
